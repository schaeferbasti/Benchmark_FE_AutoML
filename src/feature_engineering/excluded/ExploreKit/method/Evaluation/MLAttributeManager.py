import errno
import os.path
import pandas as pd
import numpy as np
from typing import Dict, List

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score

from src.feature_engineering.excluded.ExploreKit.method.Utils import FileUtils
from src.feature_engineering.excluded.ExploreKit.method.Utils.ArffManager import ArffManager
from src.feature_engineering.excluded.ExploreKit.method.Evaluation.AttributeInfo import AttributeInfo
from src.feature_engineering.excluded.ExploreKit.method.Evaluation.Classifier import Classifier
from src.feature_engineering.excluded.ExploreKit.method.Data.Dataset import Dataset
from src.feature_engineering.excluded.ExploreKit.method.Evaluation.DatasetBasedAttributes import DatasetBasedAttributes
from src.feature_engineering.excluded.ExploreKit.method.Utils.Date import Date
from src.feature_engineering.excluded.ExploreKit.method.Evaluation.EvaluationInfo import EvaluationInfo
from src.feature_engineering.excluded.ExploreKit.method.Utils.Loader import Loader
from src.feature_engineering.excluded.ExploreKit.method.Utils.Logger import Logger
from src.feature_engineering.excluded.ExploreKit.method.Evaluation.OperatorAssignment import OperatorAssignment
from src.feature_engineering.excluded.ExploreKit.method.Operators import Operator
from src.feature_engineering.excluded.ExploreKit.method.Evaluation.OperatorAssignmentBasedAttributes import OperatorAssignmentBasedAttributes
from src.feature_engineering.excluded.ExploreKit.method.Evaluation.OperatorsAssignmentsManager import OperatorsAssignmentsManager
from src.feature_engineering.excluded.ExploreKit.method.Properties import Properties
from src.feature_engineering.excluded.ExploreKit.method.Utils.Serializer import Serializer


class MLAttributeManager:
    DATASET_BASED = "DatasetBased"
    OA_BASED = "OperatorAssignmentBased"
    VALUES_BASED = "ValuesBased"
    MERGED_ALL = "merged_candidateAttributesData"
    MERGED_NO_VALUE = "merged_candidateAttributes_NoValueBased"
    ITERATION = 50000

    '''
    Generates the "background" model that will be used to classify the candidate attributes of the provided dataset.
    The classification model will be generated by combining the information gathered FROM ALL OTHER DATASETS.
    '''

    def getBackgroundClassificationModel(self, dataset: Dataset, name: str, includeValueBased: bool):
        backgroundFilePath = self.getBackgroundFilePath(dataset, name, includeValueBased)
        path = backgroundFilePath

        # If the classification model already exists, load and return it
        if os.path.isfile(path):
            Logger.Info("Background model already exists. Extracting from " + path)
            return self.getClassificationModel(dataset, backgroundFilePath)

        # Otherwise, generate, save and return it (WARNING - takes time)
        else:
            Logger.Info("Background model doesn't exist for dataset " + name + ". Creating it...")

            # We begin by getting a list of all the datasets that need to participate in the creation of the background model
            self.generateMetaFeaturesInstances(includeValueBased)

            candidateAtrrDirectories = self.getDirectoriesInFolder(Properties.DatasetInstancesFilesLocation)
            self.generateBackgroundARFFFileForDataset(dataset, name, backgroundFilePath, candidateAtrrDirectories,
                                                      includeValueBased)

            # now we load the contents of the ARFF file into an Instances object and train the classifier
            data = self.getInstancesFromARFF(backgroundFilePath)
            return self.buildClassifierModel(backgroundFilePath, data)

    def getInstancesFromARFF(self, backgroundFilePath: str) -> pd.DataFrame:
        data = Loader().readArffAsDataframe(backgroundFilePath + '.arff')
        lastColName = data.columns[-1]
        data.rename(columns={lastColName: 'class'}, inplace=True)
        Logger.Info('reading from file ' + backgroundFilePath + '.arff')
        return data

    def buildClassifierModel(self, backgroundFilePath: str, data):
        # the chosen classifier
        classifier = RandomForestClassifier()
        # classifier.setNumExecutionSlots(Integer.parseInt(properties.getProperty("numOfThreads")));

        # classifier.buildClassifier(data);
        classifier.fit(data.drop(['class'], axis=1), data['class'])
        file = backgroundFilePath + '.arff'
        FileUtils.deleteFile(file)

        Logger.Info('Saving classifier model ' + backgroundFilePath)
        self.writeClassifierTobackgroundFile(backgroundFilePath, classifier)
        return classifier

    def writeClassifierTobackgroundFile(self, backgroundFilePath: str, classifier):
        # now we write the classifier to file prior to returning the object
        Serializer.Serialize(backgroundFilePath, classifier)

    def generateBackgroundARFFFileForDataset(self, dataset: Dataset, name: str, backgroundFilePath: str,
                                             candidateAttrDirectories: list, includeValueBased: bool):
        addHeader = True
        for candidateAttrDirectory in candidateAttrDirectories:

            if (not candidateAttrDirectory.__contains__(name)) and FileUtils.listFilesInDir(
                    candidateAttrDirectory) is not None:  # none means dir exist

                merged = self.getMergedFile(candidateAttrDirectory, includeValueBased)
                if merged is not None:
                    MLAttributeManager.addArffFileContentToTargetFile(backgroundFilePath,
                                                                      FileUtils.getAbsPath(merged[0]), addHeader)
                    addHeader = False

                else:
                    instances = []  # List<Instances> instances = new ArrayList<>();
                    for file in FileUtils.listFilesInDir(candidateAttrDirectory):
                        if (('.arff' in file) and not (
                                not includeValueBased and file.contains(self.VALUES_BASED)) and not ('merged' in file)):
                            absFilePath = os.path.abspath(file)
                            instance = Loader().readArffAsDataframe(absFilePath)
                            instances.append(instance)

                        else:
                            Logger.Info(f'Skipping file: {file}')

                    mergedFile = self.mergeInstancesToFile(includeValueBased, candidateAttrDirectory, instances,
                                                           dataset.name)
                    if mergedFile is None:
                        continue
                    self.addArffFileContentToTargetFile(backgroundFilePath, FileUtils.getAbsPath(mergedFile), addHeader)
                    addHeader = False

    def mergeInstancesToFile(self, includeValueBased: bool, directory: str, instances: list, datasetName: str):
        if len(instances) == 0:
            return None

        toMerge = instances[0]
        for i in range(1, len(instances)):
            toMerge = self.mergeDataframes(toMerge, instances[i])  #Instances.mergeInstances(toMerge, instances.get(i));

        if includeValueBased:
            mergedFile = os.path.join(directory, self.MERGED_ALL + '.arff')
        else:
            mergedFile = os.path.join(directory, self.MERGED_NO_VALUE + '.arff')

        ArffManager.SaveArff(mergedFile, toMerge, datasetName)
        return mergedFile

    # TODO: replace 'mergeInstances' built-in function
    def mergeDataframes(self, first, second):
        return pd.concat([first, second], axis=1)

    def getMergedFile(self, directory: str, includeValueBased: bool):
        if includeValueBased:
            merged = filter(lambda name: self.MERGED_ALL in name, FileUtils.listFilesInDir(directory))
            merged = list(merged)
            if len(merged) == 1:
                return merged

        if not includeValueBased:
            merged = list(filter(lambda name: self.MERGED_NO_VALUE in name, FileUtils.listFilesInDir(directory)))
            if len(merged) == 1:
                return merged

        return None

    def generateMetaFeaturesInstances(self, includeValueBased: bool):
        datasetFilesForBackgroundArray = self.getOriginalBackgroundDatasets()
        for datasetForBackgroundModel in datasetFilesForBackgroundArray:
            possibleFolderName = Properties.DatasetInstancesFilesLocation + \
                                 FileUtils.getFilenameFromPath(datasetForBackgroundModel) + '_' + str(
                Properties.randomSeed)

            if not os.path.isdir(possibleFolderName):
                loader = Loader()
                Logger.Info("Getting candidate attributes for " + datasetForBackgroundModel)
                backgroundDataset = loader.readArff(datasetForBackgroundModel, int(Properties.randomSeed), None, None,
                                                    0.66)
                self.createDatasetMetaFeaturesInstances(backgroundDataset, includeValueBased)

    def getDirectoriesInFolder(self, folder: str):
        for (_, directories, _) in os.walk(folder):
            break
        if (directories is None) or (len(directories) == 0):
            raise Exception('getBackgroundClassificationModel -> no directories for meta feature instances')
        if folder not in directories[0]:
            directories = [os.path.join(folder, directory) for directory in directories]
        return directories

    # Receives a dataset object and generates an Instance object that contains a set of attributes for EACH candidate attribute
    def createDatasetMetaFeaturesInstances(self, dataset: Dataset, includeValueBased: bool):
        directoryForDataset = Properties.DatasetInstancesFilesLocation + dataset.name
        # File[] files;

        if os.path.isdir(directoryForDataset):
            _, _, filenames = next(os.walk(directoryForDataset))
            if (filenames is not None) and (len(filenames) != 0):
                Logger.Info('Candidate attributes for ' + dataset.name + ' were already calculated')
                return

        try:
            os.makedirs(directoryForDataset)
        except OSError as ex:
            if ex.errno != errno.EEXIST:
                Logger.Warn(
                    f'getDatasetMetaFeaturesInstances -> Error creating directory {directoryForDataset}\nError: {ex}')
                raise

        # List<String> metadataTypes;
        if includeValueBased:
            # This is the line that activates the (time consuming) background datasets feature generation process
            self.generateTrainingSetDatasetAttributes(dataset)
            metadataTypes = [self.DATASET_BASED, self.OA_BASED, self.VALUES_BASED]
        else:
            # for pre-ranker model
            self.generateTrainingSetDatasetAttributesWithoutValues(dataset)
            metadataTypes = [self.DATASET_BASED, self.OA_BASED]

        self.appendARFFFilesPerMetadataTypeForDataset(directoryForDataset, metadataTypes)

    def appendARFFFilesPerMetadataTypeForDataset(self, directoryForDataset: str, metaTypes: list):
        classifiers = Properties.classifiersForMLAttributesGeneration.split(',')
        seperator = os.path.sep
        for classifier in classifiers:
            for metadataType in metaTypes:
                i = 1
                fileName = directoryForDataset + seperator + classifier + "_" + metadataType + "_candidateAttributesData" + str(
                    i) + '.arff'
                targetFile = directoryForDataset + seperator + classifier + "_" + metadataType + "_candidateAttributesData" + '0'
                arffToAppendFrom = fileName
                while os.path.exists(arffToAppendFrom):
                    MLAttributeManager.addArffFileContentToTargetFile(targetFile, fileName, False)
                    FileUtils.deleteFile(arffToAppendFrom)
                    i += 1
                    fileName = f'{directoryForDataset}{seperator}{classifier}_{metadataType}_candidateAttributesData{i}.arff'
                    arffToAppendFrom = fileName

                mainFile = targetFile + '.arff'
                FileUtils.renameFile(mainFile,
                                     directoryForDataset + seperator + classifier + "_" + metadataType + "_candidateAttributesData" + '.arff')

    def getBackgroundFilePath(self, dataset: Dataset, name: str, includeValueBased: bool):
        backgroundFilePath = ''
        if includeValueBased:
            backgroundFilePath = Properties.backgroundClassifierLocation + 'background_' + name + '_' + 'DatasetBased_OperatorAssignmentBased_ValuesBased' + '_classifier_obj'
        else:
            backgroundFilePath = Properties.backgroundClassifierLocation + 'background_' + name + '_' + 'DatasetBased_OperatorAssignmentBased' + '_classifier_obj'
        return backgroundFilePath

    def getClassificationModel(self, dataset: Dataset, backgroundFilePath: str):
        try:
            backgroundModel = Serializer.Deserialize(backgroundFilePath)
            return backgroundModel

        except Exception as ex:
            Logger.Error("getBackgroundClassificationModel -> Error reading classifier for dataset " + dataset.name
                         + "  from file: " + backgroundFilePath + "  :  " + str(ex), ex)
            return None

    # return absolute path of the dataset
    def getOriginalBackgroundDatasets(self):
        datasetFolder = Properties.originalBackgroundDatasetsLocation
        datasetsFilesList = [os.path.join(datasetFolder, f) for f in os.listdir(datasetFolder) if
                             os.path.isfile(os.path.join(datasetFolder, f))]
        if len(datasetsFilesList) == 0:
            raise Exception('generateMetaFeaturesInstances -> no files in ' + datasetFolder)
        return datasetsFilesList

    # Generates the candidate attributes and then generates the meta-features for each of them. This is time consuming because you need to generate
    # all the values of each candidate features, then calculate the meta-features. Please note that the label (i.e. is the candidate attribute good or
    # bad) is dependent on the improvement obtained by re-training the model and evaluating it.
    def generateTrainingSetDatasetAttributes(self, dataset):
        Logger.Info("Generating dataset attributes for dataset: " + dataset.name)

        # DateFormat dateFormat = new SimpleDateFormat("yyyy/MM/dd HH:mm:ss");
        startDate = Date()
        # The structure: Classifier -> candidate feature (operator assignment, to be exact) -> meta-feature type -> A map of feature indices and values
        # { classifier:
        #     { OperatorAssigment:
        #           { meta-feature type: {indice, value}}
        # TreeMap<String, HashMap<OperatorAssignment,HashMap<String,TreeMap<Integer,AttributeInfo>>>> candidateAttributesList = new TreeMap<>()
        candidateAttributesList = {}

        classifiers = Properties.classifiersForMLAttributesGeneration.split(',')

        # obtaining the attributes for the dataset itself is straightforward
        dba = DatasetBasedAttributes()
        for classifier in classifiers:
            candidateAttributesList[classifier] = {}
            originalAuc = self.getOriginalAuc(dataset, classifier)

            # Generate the dataset attributes
            datasetAttributes = dba.getDatasetBasedFeatures(dataset, classifier)

            # now we need to generate the candidate attributes and evaluate them. This requires a few preliminary steps:
            # 1) Replicate the dataset and create the discretized features and add them to the dataset
            unaryOperators = OperatorsAssignmentsManager.getUnaryOperatorsList()

            # The unary operators need to be evaluated like all other operator assignments (i.e. attribtues generation)
            unaryOperatorAssignments = OperatorsAssignmentsManager.getOperatorAssignments(dataset, None, unaryOperators,
                                                                                          int(Properties.maxNumOfAttsInOperatorSource))
            replicatedDataset = self.generateDatasetReplicaWithDiscretizedAttributes(dataset, unaryOperatorAssignments)

            # 2) Obtain all other operator assignments (non-unary). IMPORTANT: this is applied on the REPLICATED dataset so we can take advantage of the discretized features
            nonUnaryOperators = OperatorsAssignmentsManager.getNonUnaryOperatorsList()
            nonUnaryOperatorAssignments = OperatorsAssignmentsManager.getOperatorAssignments(replicatedDataset, None,
                                                                                             nonUnaryOperators,
                                                                                             int(Properties.maxNumOfAttsInOperatorSource))

            # 3) Generate the candidate attribute and generate its attributes
            nonUnaryOperatorAssignments.extend(unaryOperatorAssignments)

            # oaList.parallelStream().forEach(oa -> {
            # ReentrantLock wrapperResultsLock = new ReentrantLock();
            # for (OperatorAssignment oa : nonUnaryOperatorAssignments) {

            position = [0]  #new int[]{0};

            # TODO: keep it pararell, temporary changed to single thread
            # nonUnaryOperatorAssignments.parallelStream().forEach(oa -> {
            for oa in nonUnaryOperatorAssignments:
                try:
                    datasetReplica = dataset.replicateDataset()

                    # Here we generate all the meta-features that are "parent dependent" and do not require us to generate the values of the new attribute
                    oaba = OperatorAssignmentBasedAttributes()

                    # TreeMap < Integer, AttributeInfo >
                    candidateAttributeValuesFreeMetaFeatures = oaba.getOperatorAssignmentBasedMetaFeatures(dataset, oa)

                    # ColumnInfo candidateAttribute;
                    try:
                        candidateAttribute = OperatorsAssignmentsManager.generateColumn(datasetReplica, oa, True)
                    except:
                        candidateAttribute = OperatorsAssignmentsManager.generateColumn(datasetReplica, oa, True)

                    datasetReplica.addColumn(candidateAttribute)

                    evaluationInfo = self.runClassifier(classifier, datasetReplica.generateSet(True),
                                                        datasetReplica.generateSet(False))
                    evaluationResults1 = evaluationInfo.getEvaluationStats()

                    # synchronized (this){ #TODO: part of the pararell stream
                    #     candidateAttributesList.get(classifier).put(oa, new HashMap<>());
                    candidateAttributesList[classifier][oa] = {}
                    #     candidateAttributesList.get(classifier).get(oa).put(DATASET_BASED, datasetAttributes);
                    candidateAttributesList[classifier][oa][MLAttributeManager.DATASET_BASED] = datasetAttributes
                    # Add the identifier of the classifier that was used
                    classifierAttribute = AttributeInfo("Classifier", Operator.outputType.Discrete,
                                                        self.getClassifierIndex(classifier), 3)
                    candidateAttributeValuesFreeMetaFeatures[
                        len(candidateAttributeValuesFreeMetaFeatures)] = classifierAttribute
                    candidateAttributesList[classifier][oa][
                        MLAttributeManager.OA_BASED] = candidateAttributeValuesFreeMetaFeatures

                    candidateAttributeValuesDependentMetaFeatures = oaba.getGeneratedAttributeValuesMetaFeatures(
                        dataset, oa, candidateAttribute)
                    candidateAttributesList[classifier][oa][
                        MLAttributeManager.VALUES_BASED] = candidateAttributeValuesDependentMetaFeatures
                    temp = candidateAttributesList[classifier][oa][MLAttributeManager.OA_BASED]
                    candidateAttributesList[classifier][oa][MLAttributeManager.OA_BASED][
                        len(temp)] = self.createClassAttribute(originalAuc, datasetReplica, evaluationInfo)

                    # wrapperResultsLock.lock(); #TODO: part of the pararell stream
                    if (len(candidateAttributesList[classifier]) % 1000) == 0:
                        date = Date()
                        Logger.Info(date.__str__() + ": Finished processing " + (
                                (position[0] * MLAttributeManager.ITERATION) + len(
                            candidateAttributesList[classifier]) + '/' + len(
                            nonUnaryOperatorAssignments) + ' elements for background model'))

                    if (len(candidateAttributesList[classifier]) % MLAttributeManager.ITERATION) == 0:
                        self.savePartArffCandidateAttributes(candidateAttributesList, classifier, dataset, position[0])
                        position[0] += 1
                        candidateAttributesList[classifier].clear()
                    # wrapperResultsLock.unlock(); #TODO: part of the pararell stream
                except Exception as ex:
                    Logger.Error("Error in ML features generation : " + oa.getName() + "  :  " + str(ex))

            self.savePartArffCandidateAttributes(candidateAttributesList, classifier, dataset, position[0])

        finishDate = Date()
        diffInMillies = finishDate - startDate
        Logger.Info(f"Getting candidate attributes for dataset {dataset.name} took {diffInMillies.seconds} seconds")

    def generateTrainingSetDatasetAttributesWithoutValues(self, dataset):
        Logger.Info("Generating dataset attributes for dataset: " + dataset.name)

        # DateFormat dateFormat = new SimpleDateFormat("yyyy/MM/dd HH:mm:ss");
        startDate = Date()
        # The structure: Classifier -> candidate feature (operator assignment, to be exact) -> meta-feature type -> A map of feature indices and values
        # { classifier:
        #     { OperatorAssigment:
        #           { meta-feature type: {indice, value}}
        # TreeMap<String, HashMap<OperatorAssignment,HashMap<String,TreeMap<Integer,AttributeInfo>>>> candidateAttributesList = new TreeMap<>()
        candidateAttributesList = {}

        classifiers = Properties.classifiersForMLAttributesGeneration.split(',')

        # obtaining the attributes for the dataset itself is straightforward
        dba = DatasetBasedAttributes()
        for classifier in classifiers:
            candidateAttributesList[classifier] = {}
            originalAuc = self.getOriginalAuc(dataset, classifier)

            # Generate the dataset attributes
            datasetAttributes = dba.getDatasetBasedFeatures(dataset, classifier)

            # now we need to generate the candidate attributes and evaluate them. This requires a few preliminary steps:
            # 1) Replicate the dataset and create the discretized features and add them to the dataset
            unaryOperators = OperatorsAssignmentsManager.getUnaryOperatorsList()

            # The unary operators need to be evaluated like all other operator assignments (i.e. attribtues generation)
            unaryOperatorAssignments = OperatorsAssignmentsManager.getOperatorAssignments(dataset, None, unaryOperators,
                                                                                          int(Properties.maxNumOfAttsInOperatorSource))
            replicatedDataset = self.generateDatasetReplicaWithDiscretizedAttributes(dataset, unaryOperatorAssignments)

            # 2) Obtain all other operator assignments (non-unary). IMPORTANT: this is applied on the REPLICATED dataset so we can take advantage of the discretized features
            nonUnaryOperators = OperatorsAssignmentsManager.getNonUnaryOperatorsList()
            nonUnaryOperatorAssignments = OperatorsAssignmentsManager.getOperatorAssignments(replicatedDataset, None,
                                                                                             nonUnaryOperators,
                                                                                             int(Properties.maxNumOfAttsInOperatorSource))

            # 3) Generate the candidate attribute and generate its attributes
            nonUnaryOperatorAssignments.addAll(unaryOperatorAssignments)

            # oaList.parallelStream().forEach(oa -> {
            # ReentrantLock wrapperResultsLock = new ReentrantLock();
            # for (OperatorAssignment oa : nonUnaryOperatorAssignments) {
            position = [0]  #new int[]{0};

            # TODO: keep it pararell, temporary changed to single thread
            # nonUnaryOperatorAssignments.parallelStream().forEach(oa -> {
            for oa in nonUnaryOperatorAssignments:
                try:
                    datasetReplica = dataset.replicateDataset()

                    # Here we generate all the meta-features that are "parent dependent" and do not require us to generate the values of the new attribute
                    oaba = OperatorAssignmentBasedAttributes()

                    # TreeMap < Integer, AttributeInfo >
                    candidateAttributeValuesFreeMetaFeatures = oaba.getOperatorAssignmentBasedMetaFeatures(dataset, oa)

                    evaluationInfo = self.runClassifier(classifier, datasetReplica.generateSet(True),
                                                        datasetReplica.generateSet(False))
                    evaluationResults1 = evaluationInfo.getEvaluationStats()

                    # synchronized (this){ #TODO: part of the pararell stream
                    #     candidateAttributesList.get(classifier).put(oa, new HashMap<>());
                    #     candidateAttributesList.get(classifier).get(oa).put(DATASET_BASED, datasetAttributes);
                    candidateAttributesList[classifier][oa][MLAttributeManager.DATASET_BASED] = datasetAttributes
                    # Add the identifier of the classifier that was used
                    classifierAttribute = AttributeInfo("Classifier", Operator.outputType.Discrete,
                                                        self.getClassifierIndex(classifier), 3)
                    candidateAttributeValuesFreeMetaFeatures[
                        len(candidateAttributeValuesFreeMetaFeatures)] = classifierAttribute
                    candidateAttributesList[classifier][oa][
                        MLAttributeManager.OA_BASED] = candidateAttributeValuesFreeMetaFeatures

                    # candidateAttributeValuesDependentMetaFeatures = oaba.getGeneratedAttributeValuesMetaFeatures(dataset, oa, candidateAttribute)
                    # candidateAttributesList[classifier][oa][MLAttributeManager.VALUES_BASED] = candidateAttributeValuesDependentMetaFeatures
                    candidateAttributesList[classifier][oa][MLAttributeManager.OA_BASED][
                        candidateAttributesList[classifier][oa][
                            MLAttributeManager.OA_BASED].size()] = self.createClassAttribute(originalAuc,
                                                                                             datasetReplica,
                                                                                             evaluationResults1)

                    # wrapperResultsLock.lock(); #TODO: part of the pararell stream
                    if (len(candidateAttributesList[classifier]) % 1000) == 0:
                        date = Date()
                        Logger.Info(date.__str__() + ": Finished processing " + (
                                (position[0] * MLAttributeManager.ITERATION) + len(candidateAttributesList[
                                                                                       classifier]) + '/' + nonUnaryOperatorAssignments.size() + ' elements for background model'))

                    if (len(candidateAttributesList[classifier]) % MLAttributeManager.ITERATION) == 0:
                        self.savePartArffCandidateAttributes(candidateAttributesList, classifier, dataset, position[0])
                        position[0] += 1
                        candidateAttributesList[classifier].clear()
                    # wrapperResultsLock.unlock(); #TODO: part of the pararell stream
                except Exception as ex:
                    Logger.Error("Error in ML features generation : " + oa.getName() + "  :  " + str(ex))

            self.savePartArffCandidateAttributes(candidateAttributesList, classifier, dataset, position[0])

        finishDate = Date()
        diffInMillies = finishDate - startDate
        Logger.Info(
            "Getting candidate attributes for dataset " + dataset.name + " took " + diffInMillies.seconds.__str__() + " seconds")

    # @param originalAuc to calculate the deltaauc
    # @param datasetReplica
    # @param evaluationResults1
    # @return AttributeInfo of class attribute
    def createClassAttribute(self, originalAuc: float, datasetReplica: Dataset, evaluationResults1):
        auc = self.CalculateAUC(evaluationResults1, datasetReplica.df)
        deltaAuc = auc - originalAuc
        if deltaAuc > 0.01:
            classAttribute = AttributeInfo("classAttribute", Operator.outputType.Discrete, 1, 2)
            Logger.Info("found positive match with delta " + str(deltaAuc))
        else:
            classAttribute = AttributeInfo("classAttribute", Operator.outputType.Discrete, 0, 2)
        return classAttribute

    def runClassifier(self, classifierName: str, trainingSet, testSet):
        try:
            classifier = Classifier(classifierName)
            classifier.buildClassifier(trainingSet)
            evaluationInfo = classifier.evaluateClassifier(testSet)
            # The overall classification statistics
            # Evaluation evaluation;
            # evaluation = Evaluation(trainingSet)
            # evaluation.evaluateModel(classifier, testSet)

            # The confidence score for each particular instance
            # scoresDist = []
            # for i in range(testSet.size()):
            #     testInstance = testSet[i]
            #     score = classifier.distributionForInstance(testInstance)
            #     scoresDist.append(score)

            return evaluationInfo

        except Exception as ex:
            Logger.Error("problem running classifier.", ex)

        return None

    # Replcates the provided dataset and created discretized versions of all relevant columns and adds them to the datast
    def generateDatasetReplicaWithDiscretizedAttributes(self, dataset: Dataset, unaryOperatorAssignments: list):
        replicatedDataset = dataset.replicateDataset()
        for oa in unaryOperatorAssignments:
            ci = OperatorsAssignmentsManager.generateColumn(replicatedDataset, oa, True)
            replicatedDataset.addColumn(ci)
        return replicatedDataset

    def getOriginalAuc(self, dataset: Dataset, classifier: str):
        # For each dataset and classifier combination, we need to get the results on the "original" dataset so we can later compare
        trainSet = dataset.generateSet(True)
        testSet = dataset.generateSet(False)
        originalRunEvaluationInfo = self.runClassifier(classifier, trainSet, testSet)
        # originalRunEvaluationResults = originalRunEvaluationInfo.getEvaluationStats()
        return self.CalculateAUC(originalRunEvaluationInfo, testSet)

    def CalculateAUC(self, evaluation: EvaluationInfo, dataset: pd.DataFrame) -> float:
        # auc = 0.0
        # for i in range(dataset.getNumOfClasses()):
        #     auc += evaluation.areaUnderROC(i)
        #
        # auc = auc / dataset.getNumOfClasses()
        # return auc
        return roc_auc_score(evaluation.actualPred.values, evaluation.scoreDistPerInstance[:, 1])

    # Returns an integer that is used to represent the classifier in the generated Instances object
    def getClassifierIndex(self, classifier: str) -> int:
        if classifier == 'J48':
            return 0
        elif classifier == "SVM":
            return 1
        elif classifier == "RandomForest":
            return 2
        else:
            raise Exception("Unidentified classifier")

    def savePartArffCandidateAttributes(self, candidateAttributesList: dict, classifier: str, dataset: Dataset,
                                        position: int):
        if len(candidateAttributesList[classifier]) > 0:
            filePath = Properties.DatasetInstancesFilesLocation + dataset.name
            self.saveInstancesForDatasetAttributes(filePath, candidateAttributesList, str(position))

    def saveInstancesForDatasetAttributes(self, directoryToSaveARFF: str, datasetAttributeValues: dict, part: str):
        datasetInstances = self.generateValuesMatrices(datasetAttributeValues)
        self.saveSerAndArffFiles(directoryToSaveARFF, datasetInstances, part)

    # creates and saves arff files (1 file per classifier and metadata type)
    # @param directoryForDataset directory to save the arff and ser files that will be created
    # @param datasetInstances Instances to save in files
    # @param part
    def saveSerAndArffFiles(self, directoryForDataset, datasetInstances: Dict[str, Dict[str, pd.DataFrame]], part: str):
        # for (Map.Entry<String, HashMap<String, Instances>> classifierInstances : datasetInstances.entrySet()){
        for classifierKey, classifierValue in datasetInstances.items():
            # for (Map.Entry<String, Instances> instancesByMetadataType : classifierInstances.getValue().entrySet()){
            for instancesByMetadataTypeKey, instancesByMetadataTypeValue in classifierValue.items():
                # fileName = classifierKey + "_" + instancesByMetadataTypeKey + "_candidateAttributesData" + part + ".ser"
                arffFilename = f'{directoryForDataset}{os.sep}{classifierKey}_{instancesByMetadataTypeKey}_candidateAttributesData{part}.arff'
                ArffManager.SaveArff(arffFilename, instancesByMetadataTypeValue, 'relation_name')

        #         FileOutputStream fout = new FileOutputStream(directoryForDataset.toString() + File.separator +  fileName, true);
        #         ObjectOutputStream oos = new ObjectOutputStream(fout);
        #         oos.writeObject(instancesByMetadataType.getValue());
        #         oos.close();
        #         fout.close();
        #
        #         ArffSaver saver = new ArffSaver();
        #         saver.setInstances(instancesByMetadataType.getValue());
        #         saver.setFile(new File(directoryForDataset.toString() + File.separator + classifierInstances.getKey() + "_" + instancesByMetadataType.getKey()  + "_candidateAttributesData" + part + ".arff"));
        #         saver.writeBatch();
        #
        #         //now delete the .ser file
        #         File fileToDelete = new File(directoryForDataset.toString() + File.separator + fileName);
        #         if (!fileToDelete.delete()) {
        #             LOGGER.warn("saveSerAndArffFiles -> Failed to delete file: " + fileToDelete.getAbsolutePath());
        #         }
        #     }
        # }

    # Creates  data matrices from a HashMap of AttributeInfo objects per metadata type and per classifier
    # @param datasetAttributeValues
    # @return TreeMap of classifiers -> HashMap of type of metadata of Instances
    def generateValuesMatrices(self, datasetAttributeValues: dict) -> dict:
        instancesMapPerClassifier = {}
        # for(Map.Entry<String, HashMap<OperatorAssignment, HashMap<String, TreeMap<Integer, AttributeInfo>>>> entry : datasetAttributeValues.entrySet()) {
        for attrKey, attrValue in datasetAttributeValues.items():
            instancesMapPerClassifier[attrKey] = self.generateValueMatrixPerClassifier(attrValue)

        return instancesMapPerClassifier

    def generateValueMatrixPerClassifier(self, candidateFeatureMap: Dict[
        OperatorAssignment, Dict[str, Dict[int, AttributeInfo]]]):
        valueMatrixPerMetadataType = {}
        databaseBased: List[Dict[int, AttributeInfo]] = []
        oaBased = []
        valueBased = []
        for key, val in candidateFeatureMap.items():
            databaseBased.append(val[self.DATASET_BASED])
            oaBased.append(val[self.OA_BASED])
            valueBased.append(val[self.VALUES_BASED])

        valueMatrixPerMetadataType[self.DATASET_BASED] = self._generateValuesMatrix(databaseBased, 0)
        valueMatrixPerMetadataType[self.OA_BASED] = self._generateValuesMatrix(oaBased, len(databaseBased[0]))
        valueMatrixPerMetadataType[self.VALUES_BASED] = self._generateValuesMatrix(valueBased,
                                                                                   len(databaseBased[0]) + len(
                                                                                       oaBased[0]))
        return valueMatrixPerMetadataType

    # Appends the content of an ARFF file to the end of another. Used to generate the all-but-one ARFF files
    #  * @param targetFilePath The path of the file to which we want to write
    #  * @param arffFilePath the source file
    #  * @param addHeader whether the ARFF header needs to be copied as well. When this is set to 'true' the file will be overwritten
    @staticmethod
    def addArffFileContentToTargetFile(targetFilePath: str, arffFilePath: str, addHeader: bool):
        targetFilePath += ".arff"
        fileReader = open(arffFilePath, 'r')
        if addHeader:
            fileWriter = open(targetFilePath, 'w')
        else:
            fileWriter = open(targetFilePath, 'a')

        foundData = False
        line = fileReader.readline()
        while line:
            if addHeader:
                fileWriter.write(line + '\n')
            else:
                if (not foundData) and (line.lower().find("@data") != -1):
                    foundData = True
                    line = fileReader.readline()
                if foundData:
                    fileWriter.write(line + '\n')
            line = fileReader.readline()

        fileReader.close()
        fileWriter.close()

    # An overload, used to apply the generateValuesMatrix function for a single sample
    def generateValuesMatrix(self, datasetAttributeValues: Dict[int, AttributeInfo]) -> pd.DataFrame:
        # List<TreeMap<Integer,AttributeInfo>> tempList = new ArrayList<>();
        # tempList.add(datasetAttributeValues);
        return self._generateValuesMatrix([datasetAttributeValues], 0)

    # Creates a data matrix from a HashMap of AttributeInfo objects.
    def _generateValuesMatrix(self, datasetAttributeValues: List[Dict[int, AttributeInfo]],
                              startIndex: int) -> pd.DataFrame:
        # attributes = self.generateAttributes(datasetAttributeValues[0] , startIndex)

        attributesMatrix = []
        for attValues in datasetAttributeValues:
            singleAttValues = [attValue.value for attValue in attValues.values()]
            attributesMatrix.append(singleAttValues)
        columns = [str(int(key) + startIndex) for key in datasetAttributeValues[0].keys()]
        df = pd.DataFrame(np.asarray(attributesMatrix, dtype=object))  #, columns=columns)
        #df = pd.DataFrame(np.asarray(attributesMatrix), columns=columns)
        return df
        # Instances finalSet = new Instances("trainingSet", attributes, 0);
        # for (TreeMap<Integer,AttributeInfo> attValues : datasetAttributeValues) {
        #     double[] row = new double[datasetAttributeValues.get(0).size()];
        #     for (int key : attValues.keySet()) {
        #         AttributeInfo att = attValues.get(key);
        #         if (att.getAttributeType() == Column.columnType.Numeric) {
        #             try {
        #                 Object ob = attValues.get(key).getValue();
        #                 if(ob instanceof Integer) {
        #                     row[key] = ((Integer) attValues.get(key).getValue());
        #                 }
        #                 else {
        #                     row[key] = ((Double) attValues.get(key).getValue());
        #                 }
        #             }
        #             catch (Exception ex) {
        #                 LOGGER.error("Exception: " + ex.getMessage());
        #             }
        #         }
        #         else {
        #             row[key] = (Integer) attValues.get(key).getValue();
        #         }
        #     }
        #     DenseInstance di = new DenseInstance(1.0, row);
        #     finalSet.add(di);
        # }
        # return finalSet;

    # Generates a list of Attribute objects. The types of the Attributes are in the same order as the types we
    # want to add the to Instances object.
    def generateAttributes(self, attributesData, startIndex: int):
        # ArrayList<Attribute> listToReturn = new ArrayList<>();
        # for (int i=0; i<attributesData.size(); i++) {
        #     AttributeInfo tmpAtt = attributesData.get(i);
        #     Attribute att = null;
        #     switch(tmpAtt.getAttributeType())
        #     {
        #         case Numeric:
        #             att = new Attribute(Integer.toString(startIndex+i),startIndex+ i);
        #             break;
        #         case Discrete:
        #             List<String> values = new ArrayList<>();
        #             int numOfDiscreteValues = tmpAtt.getNumOfDiscreteValues();
        #             for (int j=0; j<numOfDiscreteValues; j++) { values.add(Integer.toString(j)); }
        #             att = new Attribute(Integer.toString(startIndex+i), values, startIndex + i);
        #             break;
        #         case String:
        #             //Most classifiers can't handle Strings. Currently we don't include them in the dataset
        #             break;
        #         case Date:
        #             //Currently we don't include them in the dataset. We don't have a way of handling "raw" dates
        #             break;
        #         default:
        #             throw new Exception("unsupported column type");
        #     }
        #     if (att != null) {
        #         listToReturn.add(att);
        #     }
        # }
        pass

        # return listToReturn;
