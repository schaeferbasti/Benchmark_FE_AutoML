#!/bin/bash

# Define the partition on which the job shall run.
#SBATCH --partition xxx

# Define a name for your job
#SBATCH --job-name Autogluon_Pipeline

# Define the files to write the outputs of the job to.
#SBATCH --output logs/%x-%A.out   # STDOUT  %x and %A will be replaced by the job name and job id, respectively. short: -o logs/%x-%A.out
#SBATCH --error logs/%x-%A.err    # STDERR  short: -e logs/%x-%A.out

# Define the amount of memory required per node
#SBATCH --mem 32GB
#SBATCH --cpus-per-task=8
#SBATCH --gres=localtmp:100

#SBATCH --propagate=NONE # to avoid a bug

echo "Workingdir: $PWD";
echo "Started at $(date)";

# A few SLURM variables
echo "Running job $SLURM_JOB_NAME using $SLURM_JOB_CPUS_PER_NODE cpus per node with given JID $SLURM_JOB_ID on queue $SLURM_JOB_PARTITION";

# Activate your environment
# You can also comment out this line, and activate your environment in the login node before submitting the job
# shellcheck disable=SC1090
source ~/miniconda3/bin/activate # Adjust to your path of Miniconda installation
if conda info --envs | grep -q amltk_env; then echo "autogluon_env already exists"; else conda create -y -n autogluon_env; fi
conda activate autogluon_env
echo "conda autogluon_env activated"

# Install requirements
python3 -m pip install --upgrade pip
pip install -r requirements.txt
echo "Requirements installed"

# Set the PYTHONPATH to include the src directory
export PYTHONPATH=$PWD/src:$PYTHONPATH
echo "PYTHONPATH set to $PYTHONPATH"

# Running the job
# shellcheck disable=SC2006
start=`date +%s`

# shellcheck disable=SC2048
python3 src/autogluon/run_autogluon.py "$SLURM_ARRAY_TASK_ID" "$*"

# shellcheck disable=SC2006
end=`date +%s`
runtime=$((end-start))

echo "Job execution complete."
echo "Runtime: $runtime"
